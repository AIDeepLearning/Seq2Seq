#!/usr/bin/env bash
cd ../..
python3 train.py\
    --model_class seq2seq_attention\
    --batch_size 256\
    --hidden_units 400\
    --embedding_size 300\
    --attention_units 250\
    --encoder_depth 3\
    --decoder_depth 3\
    --encoder_max_time_steps 100\
    --decoder_max_time_steps 25\
    --display_freq 5\
    --save_freq 2000\
    --valid_freq 400\
    --model_dir checkpoints/lcsts_bpe_seq2seq_attention\
    --model_name lcsts.ckpt\
    --source_vocabulary dataset/lcsts/bpe/vocabs.json\
    --target_vocabulary dataset/lcsts/bpe/vocabs.json\
    --source_train_data dataset/lcsts/bpe/sources.train.txt\
    --target_train_data dataset/lcsts/bpe/summaries.train.txt\
    --source_valid_data dataset/lcsts/bpe/sources.eval.txt\
    --target_valid_data dataset/lcsts/bpe/summaries.eval.txt\
    --encoder_vocab_size 27005\
    --decoder_vocab_size 27005\
    --cell_type gru\
    --max_epochs 100000